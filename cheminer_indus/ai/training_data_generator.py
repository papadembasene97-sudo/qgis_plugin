"""
Utilitaire pour g√©n√©rer des donn√©es d'entra√Ænement depuis l'historique de visites
"""

import json
from datetime import datetime, timedelta
from typing import List, Dict
import random


def convert_visits_to_training_data(visits_history: List[Dict], 
                                    canal_layer,
                                    ouvr_layer) -> List[Dict]:
    """
    Convertit l'historique des visites en donn√©es d'entra√Ænement pour l'IA
    
    Args:
        visits_history: Liste des visites (format interne du plugin)
        canal_layer: Couche des canalisations QGIS
        ouvr_layer: Couche des ouvrages QGIS
        
    Returns:
        Liste de donn√©es format√©es pour l'entra√Ænement
    """
    training_data = []
    
    for visit in visits_history:
        node_id = visit.get('node_id')
        polluted = visit.get('polluted', False)
        visit_date = visit.get('date', datetime.now())
        
        # R√©cup√©ration du n≈ìud (ouvrage)
        node_feature = ouvr_layer.getFeature(node_id)
        
        if not node_feature:
            continue
        
        node_geom = node_feature.geometry()
        node_point = node_geom.asPoint()
        
        node_data = {
            'id': node_id,
            'x': node_point.x(),
            'y': node_point.y(),
            'elevation': node_feature.attribute('altitude') or 0
        }
        
        # Recherche des branches amont
        upstream_data = []
        expr = f"trim(idnterm) = '{node_id}'"
        canal_layer.selectByExpression(expr)
        
        for canal_feat in canal_layer.selectedFeatures():
            upstream_data.append({
                'diametre': canal_feat.attribute('diametre') or 300,
                'pente': canal_feat.attribute('pente') or 0.005,
                'z_amont': canal_feat.attribute('z_amont') or 0,
                'z_aval': canal_feat.attribute('z_aval') or 0,
                'longueur': canal_feat.geometry().length(),
                'type_reseau': canal_feat.attribute('type_reseau') or 'EU',
                'materiau': canal_feat.attribute('materiau') or 'PVC'
            })
        
        # Recherche des branches aval
        downstream_data = []
        expr = f"trim(idnini) = '{node_id}'"
        canal_layer.selectByExpression(expr)
        
        for canal_feat in canal_layer.selectedFeatures():
            downstream_data.append({
                'diametre': canal_feat.attribute('diametre') or 300,
                'pente': canal_feat.attribute('pente') or 0.005,
                'z_amont': canal_feat.attribute('z_amont') or 0,
                'z_aval': canal_feat.attribute('z_aval') or 0,
                'longueur': canal_feat.geometry().length(),
                'type_reseau': canal_feat.attribute('type_reseau') or 'EU',
                'materiau': canal_feat.attribute('materiau') or 'PVC'
            })
        
        # Contexte historique (visites pr√©c√©dentes)
        historical_context = [
            {
                'node_id': v.get('node_id'),
                'polluted': v.get('polluted', False),
                'date': v.get('date', datetime.now())
            }
            for v in visits_history
            if v.get('date', datetime.now()) < visit_date
        ]
        
        # Ajout de l'√©chantillon
        training_sample = {
            'node_data': node_data,
            'upstream_data': upstream_data,
            'downstream_data': downstream_data,
            'historical_context': historical_context,
            'polluted': polluted,
            'date': visit_date.isoformat() if isinstance(visit_date, datetime) else visit_date
        }
        
        training_data.append(training_sample)
    
    canal_layer.removeSelection()
    
    return training_data


def generate_synthetic_training_data(nb_samples: int = 100) -> List[Dict]:
    """
    G√©n√®re des donn√©es synth√©tiques pour tester l'IA
    (√Ä utiliser uniquement pour le d√©veloppement/test)
    """
    import numpy as np
    
    training_data = []
    base_date = datetime.now() - timedelta(days=365)
    
    for i in range(nb_samples):
        # Simulation d'un n≈ìud
        node_data = {
            'id': f'node_{i}',
            'x': random.uniform(0, 1000),
            'y': random.uniform(0, 1000),
            'elevation': random.uniform(50, 150)
        }
        
        # Simulation branches amont (1 √† 4)
        nb_upstream = random.randint(1, 4)
        upstream_data = []
        
        for j in range(nb_upstream):
            upstream_data.append({
                'diametre': random.choice([200, 300, 400, 500, 600]),
                'pente': random.uniform(0.002, 0.02),
                'z_amont': node_data['elevation'] + random.uniform(0.5, 3),
                'z_aval': node_data['elevation'],
                'longueur': random.uniform(10, 100),
                'type_reseau': random.choice(['EU', 'EP', 'Mixte']),
                'materiau': random.choice(['PVC', 'Fonte', 'B√©ton'])
            })
        
        # Simulation branches aval (1 √† 2)
        nb_downstream = random.randint(1, 2)
        downstream_data = []
        
        for j in range(nb_downstream):
            downstream_data.append({
                'diametre': random.choice([300, 400, 500, 600, 800]),
                'pente': random.uniform(0.002, 0.02),
                'z_amont': node_data['elevation'],
                'z_aval': node_data['elevation'] - random.uniform(0.5, 3),
                'longueur': random.uniform(10, 100),
                'type_reseau': random.choice(['EU', 'EP', 'Mixte']),
                'materiau': random.choice(['PVC', 'Fonte', 'B√©ton'])
            })
        
        # Simulation historique (0 √† 5 visites pr√©c√©dentes)
        nb_history = random.randint(0, 5)
        historical_context = []
        
        for j in range(nb_history):
            historical_context.append({
                'node_id': f'node_{random.randint(0, i)}',
                'polluted': random.random() > 0.7,  # 30% de pollution
                'date': (base_date + timedelta(days=j*30)).isoformat(),
                'x': node_data['x'] + random.uniform(-50, 50),
                'y': node_data['y'] + random.uniform(-50, 50)
            })
        
        # Logique de pollution (r√®gles simplifi√©es)
        # Plus de probabilit√© si:
        # - R√©duction de diam√®tre importante
        # - Pente faible
        # - Historique de pollution dans le voisinage
        
        avg_diameter_up = np.mean([u['diametre'] for u in upstream_data])
        avg_diameter_down = np.mean([d['diametre'] for d in downstream_data])
        diameter_reduction = avg_diameter_up > avg_diameter_down * 1.2
        
        low_slope = np.mean([u['pente'] for u in upstream_data]) < 0.005
        
        nearby_polluted = len([h for h in historical_context if h['polluted']])
        
        # Calcul probabilit√©
        pollution_score = 0
        if diameter_reduction:
            pollution_score += 0.3
        if low_slope:
            pollution_score += 0.2
        pollution_score += nearby_polluted * 0.1
        
        polluted = random.random() < pollution_score
        
        training_sample = {
            'node_data': node_data,
            'upstream_data': upstream_data,
            'downstream_data': downstream_data,
            'historical_context': historical_context,
            'polluted': polluted,
            'date': (base_date + timedelta(days=i*3)).isoformat()
        }
        
        training_data.append(training_sample)
    
    return training_data


def save_training_data(data: List[Dict], output_path: str):
    """Sauvegarde les donn√©es d'entra√Ænement"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"üíæ Donn√©es d'entra√Ænement sauvegard√©es: {output_path}")
    print(f"   Nombre d'√©chantillons: {len(data)}")
    print(f"   Pollu√©s: {sum(1 for d in data if d['polluted'])}")


def load_training_data(input_path: str) -> List[Dict]:
    """Charge les donn√©es d'entra√Ænement"""
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"üìÇ Donn√©es d'entra√Ænement charg√©es: {input_path}")
    print(f"   Nombre d'√©chantillons: {len(data)}")
    
    return data


if __name__ == '__main__':
    # Exemple: g√©n√©rer des donn√©es synth√©tiques pour test
    print("ü§ñ G√©n√©ration de donn√©es d'entra√Ænement synth√©tiques...")
    
    synthetic_data = generate_synthetic_training_data(nb_samples=200)
    save_training_data(synthetic_data, 'training_data_synthetic.json')
    
    print("\n‚úÖ Donn√©es g√©n√©r√©es avec succ√®s !")
    print("   Vous pouvez maintenant les utiliser pour entra√Æner le mod√®le IA")
